\specialchapt{ABSTRACT}

%!TEX root = ../dissertation.tex
In many instances, it is useful to view data as a collection of curves. The study of human growth is a common example used to motivate this view, where data arising from the study can be viewed as collection of individual growth curves.  The second derivative of the growth curves are used to study growth acceleration and identify common growth spurts. In studies like this, where the questions motivating the analysis relate to properties of curves, it makes sense to view the fundamental datum as a curve and refer to the collection of curves as \emph{functional data}.  \cite{ferraty2006nonparametric} characterize this by defining a functional random variable as one which takes values in an infinite dimensional space. 

The publication of the book \emph{Functional Data Analysis}---the seminal work by \cite{FDA}---inspired substantial interest in developing statistical models for functional data.  Though their book describes the extension of linear models to the functional setting, it stops short of allowing for complex dependence structures by assuming independence between observed curves. For curves observed in space, this assumption is often violated. Examples of functional data which have a spatial index are becoming more common in scientific studies: data collected by weather stations, oceanology measurements taken by seals equipped with sensor devices, peak electron density measurements collected by ionosonde stations in the northern hemisphere, and satellite measurements of environmental variables. Therefore,  research on functional models for spatial data is a very important and actual topic.

 In environmental data,  the correlations exhibited between observations in space may be of scientific interest and often the primary goal of the analysis is prediction of a environmental variable at an unobserved location.  In this setting, geostatistical models are often used. The extension of geostatistical models for functional data were first considered by \cite{Goulard:1993}, who proposed two possible approaches: one approach involves cokriging by reducing the functional response to a multivariate response, while the other approach untilizes a functional version of the variogram.  The functional variogram approach has been further developed by \cite{Giraldo:2010jx} and \cite{Nerini:2010ba}.

%Examples of spatially indexed functional data have also been considered for functional data with a multilevel structure, often arising in the case of designed experiments. In these cases, a mixed model framework is typically used and much work has been done extending mixed model methodology to accommodate functional observations (\cite{Li:2007dn}, \cite{Morris:2006wq},\cite{Di:2009dz}, \cite{Baladandayuthapani2008},\cite{Staicu:2010ez}, \cite{Scheipl:2012tm}). 

When data are random curves rather than scalars or vectors, it is necessary to have a convenient way to represent the curves.  A standard approach is to express the curves in terms of a known basis set (e.g. b-spline, wavelets, Fourier). An alternative, and what we believe to be a more attractive choice for spatial functional data, is an empirical basis consisting of functional principal components (FPC). When curves are represented by a linear combination of basis functions, the variation among curves can be investigated through the variation among the coefficients of the basis functions. Thus, spatial similarity between curves can be modeled through the correlation structure associated with the basis function coefficients viewed as scalar random fields. Using a finite basis representation of the curves, each curve is identified by its vector of coefficients, allowing the use of multivariate methods; moreover, it is well known that principal components achieve maximum efficiency in terms of representing variation in the data for a fixed dimension. 

To clarify what is meant by functional principal components, we present here a brief review of the relevant mathematical results.
A process $X(t)$, defined on a closed interval $\T  \subset \Real$,  with finite covariance possesses a sequence of orthonormal eigenfunctions $\{\psi_m(t)\}_{m=1,2,\ldots}$, which form a complete basis for the space, with associated nonnegative and nondecreasing eigenvalues $\{\lambda_m \}_{m=1,2,\ldots}$. By the Karhunen-Loeve expansion, the process $X(t)$ admits the representation
\begin{equation*}
X(t) =  \sum_{m=1}^{\infty}\alpha_m \psi_m(t), \mbox{ where  } \alpha_m = \int_{\T} X(t) \psi_m(t)dt.
\end{equation*}

The random variables $\{\alpha_m \}_{m=1,2,\ldots}$ are the functional principal component scores of $X(t)$, which are uncorrelated and satisfy $E(\alpha_m)=0$ and var($\alpha_m$) = $\lambda_m$, $\sum_m \lambda_m < \infty$.\\
Remarks:
\begin{enumerate}
	\item The eigenfunctions are orthonormal in the space $L^2[\T]$ and the FPC scores are uncorrelated random variables.
	\item The eigenfunctions efficiently capture the dominant modes of variation in the observed curves.
	\item The eigenvalues often decrease rapidly and thus the infinite dimensional process $X(t)$ can be well approximated by a small number of FPCs.
\end{enumerate}

This framework is often adopted (e.g., \cite{Yao:2005cv,Di:2009dz,Gromenko:2012ij}); however, 
this approach introduces the non-trivial problem of estimating the principal component functions from observed data. Estimation of principal component functions can be accomplished by estimating the covariance function through bivariate smoothing and then computing the eigenfunctions of the estimated covariance surface (some alternative methods are described in \cite{FDA} Ch. 8 and Ch. 9). Because of this, much effort has been devoted to methods for nonparametric covariance estimation and eigenfunction estimation ( \cite{Yao:2005cv}, \cite{Li:2007dn}, \cite{Cai:2010vr}).  Of the literature on nonparametric covariance function estimation, \cite{Cai:2010vr} provides the most general framework by assuming curves belong to a reproducing kernel Hilbert space (RKHS) and utilizing theoretical results developed for spline smoothing methods. This framework allows for closed form estimation of the eigenfunctions and their derivatives, so one does not need to discretize the covariance function and approximate eigenfunctions with eigenvectors.  This is particularly useful because we use eigenfunctions as a basis set for further model building and not just as an exploratory tool. 

In this dissertation we work in a reproducing kernel Hilbert space framework, representing curves with a principal component function basis, and model spatial dependence between curves through FPC scores.  
\begin{itemize}
\item In Chapter \ref{ch:covariance estimation} we describe an estimator for principal component functions by extending the methods developed in \cite{Cai:2010vr}. We have developed an R implementation of this method. In our implementation one can easily produce list object containing the estimated principal component functions. We have also developed an empirical basis specification  within the fda package framework, so that the estimated principal component functions can be used as a basis object.  %Ramsay and Silverman have stated that a future fda release will contain support for empirical bases, but I do not believe this has been implemented yet. 
\item In Chapter \ref{ch:functional kriging} we describe how this framework can be used for ordinary kriging prediction for functional data. We also describe our current work on improving the nonparametric covariance estimator by incorporating spatial dependence in the estimation of the covariance function. 
\item  In Chapter \ref{future work} we discuss future directions in model development motivated by spatial functional data arising from a phenological study in India. Some of the scientific questions related to these data involve identifying biological life-cycles, which can be cast in terms of derivatives of the curves (e.g., global mixima, inflection points). This fact further motivates the functional data approach, as it is a natural way in which one can estimate and represent derivatives. 
\item In order to make this document more self contained, Appendix \ref{ch:theoretical background} contains some of the theoretical background necessary to understand the developments in Chapter \ref{ch:covariance estimation}. 
\end{itemize}

%
%In functional data analysis a datum is considered to be a function defined on a domain $\T$ and a data set consists of a collection of functions.  The collection of curves can be conceptualized as realizations from a stochastic process. In practice each curve is observed at finitely many points and with noise, thus a regularization procedure --- converting raw data to smooth curves --- is typically an initial step. A popular regularization approach involves expressing the curves in terms of a finite basis (e.g. Fourier, wavelet, b-spline). An attractive alternative is an empirical basis consisting of principal component functions. Principal component functions are the infinite dimensional version of principal component vectors in that they define a coordinate system aligned with the major modes of variability within the data. Deriving principal component functions---similarly to deriving principal component vectors---involves an eigen-analysis of the covariance operator.  
%
%In functional data analysis the covariance operator is not a matrix, but a bivariate function defined on a rectangle. Eigen-analysis of a bivariate function is not as straightforward as it matrix counterpart, but we adopt a method proposed by \cite{Cai2010} who, by working in a reproducing kernel Hilbert space framework, give formulas for closed form estimates of the eigenfunctions. In their method, covariance function estimation is approached as a bivariate smoothing problem. Assuming the curves belong to a RKHS naturally identifies the covariance function as belonging the the tensor product reproducing kernel Hilbert space. The estimator of the covariance function is cast as a penalized regression problem utilizing the norm on the tensor product space. It is has been shown that the estimator has a finite dimensional representation, reducing the eigenfunction problem to an eigenvector problem.

%\end{spacing}

