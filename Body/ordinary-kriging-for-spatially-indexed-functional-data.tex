

%!TEX root = ../dissertation.tex
\chapter{ESTIMATION AND KRIGING FOR SPATIALLY INDEXED FUNCTIONAL DATA} 
\label{ch:functional kriging}

\section{Introduction} 

% (fold)
\label{sec:introduction}
Data that arise from measurements at geographic locations often exhibit similarities at small spatial scales. When the primary objective of analysis is to predict values at unobserved locations, geostatistical models are particularly useful. When data are measurements from a ground-based sensor, there is often temporal component to the data, examples include: \cite{Kaiser:2002wna} who investigate the concentration of an air pollutant; Delaigle and Hall (2010) who consider Australian rainfall data; and the Canadian weather data showcased in Ramsay and Silverman (2005). When the temporal measurements are not regular in time its useful to treat the time process as functional data. The extension of geostatistical models for functional data were first considered in \cite{Goulard:1993} where two approaches are proposed: one approach involves cokriging by reducing the functional response to a multivariate response by modeling the spatial component through the coefficients of a parametric model, while the other approach utilizes a functional version of the variogram. The functional variogram approach has been further developed by \cite{Giraldo:2010jx}. We pursue the first option, but do not make the assumption of a parametric model for the curves. We allow the curves to be represented nonparametrically using a principal component function basis. Typically, very few principal component functions are needed to represent the major modes of variation in the curves, thus making a multivariate geostatistical approach feasible without the need of a parametric model. 



% \begin{figure}
% 	\begin{center}
% 		\includegraphics[width=0.7\textwidth]{images-ordinary-kriging/plot_from_Nerini2010.png}
% 	\end{center}
% 	\caption{Data set from \cite{Nerini:2010ba} modeling ocean temperatures collected by equipment attached to diving elephant seals.} \label{fig:nerini}
% \end{figure}

% section introduction (end)

% \section{Statistical model for spatially indexed functional data}
%
% % (fold)
% \label{sec:statistical_model_for_spatially_indexed_functional_data}
%
% We work with data consisting of curves $X(s_k; t),$ $t \in [0,T]$, at spatial locations $s_1, \dots, s_n$. We define a \emph{spatial functional process} as
% \[ \left\{ \boldsymbol X(s; t): s \in D \subseteq \Real^2, t \in \T \right\}, \]
% where, for fixed location $s_k$, $\boldsymbol X(s_k; \cdot)$ is a random function on the closed interval $\mathcal{T}$ taking values in a reproducing kernel Hilbert space of functions $\H$ with reproducing kernel $K(s,t)$ which is assumed to be square integrable. The requirement that the curves belong to a reproducing kernel Hilbert space instead of typical $L^p$ space has to do with the method we employ to estimate the principal component functions in section \ref{sec:eigenfunction_estimation}.
%
% The functions $X(s;t)$ admit the following representation
% \begin{equation}
% 	X(s;t) = \mu(t) + \epsilon(s;t),
% \end{equation}
% where $\mu(t)$ represents large-scale structure which does not depend on spatial location and $\epsilon(s;t)$ is a mean zero spatially correlated random effect. The methodology we propose assumes $\epsilon(s;t)$ can be represented by the Karhunen-Loeve expansion $\epsilon(s;t) = \sum_{k=1}^{\infty} \alpha_k(s)\psi_k(t)$. The functions $\psi_k(\cdot)$ are eigenfunctions of the covariance operator and are called the principal component functions (FPC). For each integer $k$, $\alpha_k(s) = \inner{\epsilon(s;t)}{ \psi_k(t)}$ is a scalar random field assumed to be a second-order stationary and isotropic. Spatial random fields connected to different FPCs are assumed to be uncorrelated, that is
% \begin{equation}
% 	\text{Cov}(\alpha_j(s), \alpha_l(s')) = 0 \hspace{1cm} \text{for } j \neq l. \label{eq:nocrosscor}
% \end{equation}
%
% For simplicity assume the functions are centered (i.e. $\mu(t)=0$), then under assumption \eqref{eq:nocrosscor}, the covariance between trajectories at locations $s_j$ and $s_l$ are given by
% \begin{align}
% 	\text{Cov}(X(s_j,t), X(s_l, t')) &= \sum_{k=1}^{\infty}\text{Cov}(\alpha_k(s_j), \alpha_k(s_l))\psi_k(t)\psi_k(t')\\
% 	&= \sum_{k=1}^{\infty}h_k(\norm{s_j-s_l})\psi_k(t)\psi_k(t'). \label{eq:cov}
% \end{align}
% Gromenko and Kokozka (2012) point out that assumption \eqref{eq:nocrosscor} is implied by a separable spatio-temporal covariance function, and is necessary to ensure positive definiteness. Further, it is clear from the form of the covariance function \eqref{eq:cov} that the spatial component is stationary, while the temporal component is nonstationary. From a practical viewpoint, assumption \ref{eq:nocrosscor} results in a fairly parsimonious model as it does not require modeling the cross covariances between coefficient random fields.
%
% In practice we work with the truncated expansion $\epsilon(s;t) = \sum_{k=1}^{q} \alpha_k(s)\psi_k(t)$, where $q$ is chosen to preserve most of the variation. Conventional wisdom stipulates that this means at least 85\% of the total variation is accounted for by the truncated expansion. Our experience suggests that often no more then 2-4 FPCs are need to capture 85\% of the variation, so this approach often reduces to a very low dimensional multivariate problem.
%
% % section statistical_model_for_spatially_indexed_functional_data (end)


\section{Ordinary kriging of function-valued data} % (fold)
\label{sec:ordinary_kriging_of_function_valued_data}
The ordinary kriging methodology described in this section was developed in \cite{Giraldo:2010jx}, and is a direct translation of the ordinary kriging from the scalar-valued data to function-valued data. Let $X_{s_1}(t), \dots, X_{s_n}(t)$ be realizations of the functional random process $X_s(t)$ at site $s_1, \dots, s_n$. For an unobserved location $s_0$, the predictor for $X_{s_0}$ is given by 
\begin{equation}
	\hat{X}_{s_0} = \sum_{i=1}^n\lambda_i X_{s_i}(t) \mbox{\hspace{0.5cm}} \lambda_i, \dots, \lambda_n \in \mathbb{R} \label{OKFD predictor}
\end{equation}
The following formal assumptions establish the stationarity conditions:
\begin{itemize}
	\item $E(X_s(t)) = \mu(t)$ and $Var(X_s(t)) = \sigma^2(t)$ for all $s \in D$ and $t \in [a,b]$
	\item $Cov(X_{s_i}(t), X_{s_j}(t)) = C(\norm{s_i - s_j})(t) = C_{ij}(h,t)$, where $h = \norm{s_i - s_j}$.
	\item $\frac{1}{2}Var(X_{s_i} - X_{s_j}) = \gamma(\norm{s_i - s_j})(t) = \gamma(h,t)$
\end{itemize}
The estimator in \eqref{OKFD predictor} is a linear predictor and the weights $\lambda_i$ are derived such that the predictor is the best linear unbiased predictor (BLUP). The unbiased constraint requires that $\sum_{i=1}^n\lambda_i = 1$, and the BLUP is obtained by minimizing
\begin{equation}
	\sigma^2_{s_0} = Var(\hat{X}_{s_0} - X_{s_0}).
\end{equation}
Implementation of this method requires a preprocessing step involving non-parametric fitting of the observed data to achieve smooth representations of the functions $X_{s_1}(t), \dots, X_{s_n}(t)$. This is accomplished by fitting a finite b-spline basis where the dimension of the basis and the smoothing parameter are chosen by a functional cross-validation algorithm. 
% section ordinary_kriging_of_function_valued_data (end)

\section{Cokriging functional principal components scores} % (fold)
\label{sec:cokriging_functional_principal_compents_scores}
In the method described in the previous section, the set of curves are modeled directly as a spatial random field. The approach we describe here is fundamentally different in that the spatial model is defined on the coefficient vectors corresponding to a finite basis expansion. The coefficient vector is often low dimensional, as each curve is represented as a linear combination of the leading functional principal components. To guarantee such a representation exists and can be estimated efficiently requires assumptions on the function space itself, which we describe here. 
  We model trajectories $X_s(t)$ as a continuos second order stochastic process with mean $\mu(t)$ and bivariate temporal covariance function
\begin{equation}
	C_{0}(t',t)=E([X(t')-\mu(t')][X(t)-\mu(t)]),\mbox{ }\forall t',t\in \T=[a,b]. 
\end{equation} 
 We assume the underling trajectories $X_s(t)$ are smooth in that they take values in the reproducing kernel Hilbert space $\H= \{f : f, f' \mbox{ absolutely continuous}, f'' \in L_2[\T]\}$, with inner product $\inner{f}{g}_{\H}$ described in \eqref{inner prod}.

Let $X_{s_1}(t), \dots, X_{s_n}(t)$ represent the collection of realizations of $X_s(t)$. The data we consider are finite observations of each curve corrupted by noise. The model for the observed values $Y_s(t_{ij})$ is described by,
\begin{equation}
	Y_s(t_{ij})=X_s(t_{ij})+\epsilon_{ij},\mbox{ }j=1,\dots,m;\mbox{ }i=1,\dots,N, \label{kriging:observation model}
\end{equation} 
where $i$ indexes spatial location and $j$ indexes the finite observations on a single trajectory. The `time' observations are not assumed to be the same across curves; that is, $t_{ij}$ does not necessarily equal $t_{i'j}$ for $i \neq i'$. To simplify notation we assume the number of observations on each curve, $m$, is consistent across curves though this assumption can be relaxed. The $\epsilon_{ij}$ are independently and identically distributed measurement errors with mean zero and finite variance $\sigma_{0}^{2}.$ It is further assumed that the random functions $X_s(t)$, and measurement errors $\epsilon$ are mutually independent. 

To simplify notation, assume $\mu(t)=0$, so that functions $X_s(t)$ admit the following representation 
\begin{equation}
	X_{s}(t) = \sum_{k=1}^{q} \alpha_k(s)\psi_k(t) = \boldsymbol{\alpha}\boldsymbol{\psi}, \label{kriging: fpc expansion}
\end{equation}
where the functions $\psi_k(t)$ are eigenfunctions of the covariance function $C_0(t',t)$, and $\alpha_k(s) = \inner{X_s(t)}{ \psi_k(t)}_{\H}$. The value of $q$ is determined such that at least 90\% of the variation in the curves is explained by the first $q$ FPCs. In this framework, predicting a curve at an unobserved location $s_0$ is achieved by predicting the corresponding coefficient vector $\boldsymbol{\alpha(s_0)}=[\alpha_1(s_0), \dots, \alpha_q(s_0)]$. This approach requires two separate tasks: estimating the functional principal components, and modeling the coefficient random fields. Section \ref{sub:FPC estimation} describes our method for estimating FPCs, and Section \ref{sec:functional_kriging_predictor} describes our approach to prediction by modeling the coefficients as a multivariate spatial random field.
% subsection subsection_name (end)
\subsection{Estimation} % (fold)
\label{sub:estimation}

% subsection estimation (end)
We achieve smooth versions of the underlying trajectories $X_s(t)$ by expressing them as a finite basis expansion of principal component functions corresponding the the covariance function $C_0(t',t)$. The covariance of the observational process $Y_{ij}$ is given by
\begin{equation}
	C(Y_i(t_{ij}), Y_i(t_{ik})) = C_0(X_i(t_{ij}), X_i(t_{ik})) + \sigma^2_0 \delta_{jk}
\end{equation}
where $\delta_{jk}$ equals 1 if $j=k$ and is equal to zero otherwise. The covariance function $C_0(t',t)$ is recovered by performing bivariate smoothing on the sample covariance omitting the diagonal values.  The covariance function estimator we use is described in Chapter \ref{ch:covariance estimation} and is a modified version of the one proposed in \cite{Cai:2010vr} who show that this method has many desirable theoretical properties. The methodology, which is described in detail in described in Chapter \ref{ch:covariance estimation}, achieves both efficient estimation of $C_0(t',t)$ and results in closed-form estimates of corresponding FPCs.

\subsubsection{Covariance Estimation for independent curves} % (fold)
\label{sub:subsection_name}

% subsection subsection_name (end)
In this section we describe a nonparametric estimator for the covariance function $C_0(t',t)$ based on a collection $X_{s_1}(t), \dots, X_{s_n}(t)$ of independent realizations of the functional process $X_s(t)$. Let $\mathbf{b}^{(i)} = [(Y_i(t_{ij})-\mu(t_{ij}))(Y_i(t_{ij'})-\mu(t_{ij'}))]_{1\leq j\neq j'\leq m}$, $i=1, \dots, N$; $j,j' \in 1, \dots, m$. Further, let
\begin{equation}
	\mathbf{b} = (\mathbf{b}^{(1)T}, \mathbf{b}^{(2)T}, \dots, \mathbf{b}^{(n)T} )^T, \label{b}
\end{equation} 
where the column vectors $\mathbf{b}^{(i)}$ contain all pairwise products of observations on the $i$th curve, excluding those that are the product of an observation with itself which correspond to the diagonal values on $[0,1]\times [0,1]$. The column vector $\mathbf{b}$ contains all the information in the sample about the covariance function. Using this notation the covariance estimator is defined by the following optimization problem,
\begin{equation}
	 \widehat{C}_{\lambda}=\stackrel[C \in \H\otimes \H]{}{\text{ argmin}} \left\{\frac{1}{nm^2-nm} (\mathbf{b} - \mathbf{C})^T(\mathbf{b} - \mathbf{C})+\lambda\left\Vert C\right\Vert _{\breve{\H}}^{2} \right\},
	 \label{kriging: cov est}
	 \end{equation}
where
\[ \mathbf{C} = [C(t_{ij}, t_{ij'})], \]
 $\lambda$ is a smoothing parameter estimated using cross validation, and $\breve{\H}$ is a subspace of $\H\tprod\H$. Details about the Hilbert space structure of $\H\tprod\H$ and form of the estimator see Chapter \ref{ch:covariance estimation}. 

\subsubsection{Covariance Estimation for spatially dependent curves} % (fold)
\label{sub:weighted covariance}

The covariance function estimator \eqref{kriging: cov est} assumes independent observations. Using this estimator with spatially correlated data may have an affect on bias, variance, or result in an estimator that is not consistent. In this section we introduce a computationally efficient way to adjust for some of the effects of spatial dependence on the covariance estimator \ref{kriging: cov est} using a weighting scheme motivated by the fundamental principle in spatial statistics: data in closer proximity have stronger correlation and contribute similar information. For irregularly spaced data, this can cause a bias toward the behavior of observations that are clustered together and are over represented in the sample. Here we describe an approach to counteract this tendency by down weighting curves where location intensity (i.e., number of locations per unit area) is high. Previous work on smoothing penalty based estimator like the one in \eqref{kriging: cov est} has shown that when the dependence assumption is violated, the estimator tends to under-smooth (\cite{Wang:1998tq}). The approach we consider here does not address the selection of the smoothing parameter directly, but reduces the influence of the highly correlated observations on estimation. 

Our approach involves creating a scalar weight for each curve based on location intensity and the strength of the correlation. The goal is to down-weight observations from highly correlated data because they provide redundant information. This approach is computationally efficient because it does not require computing the inverse of a high dimensional matrix---which is not feasible with the dimension of $\mathbf{b}$ in \eqref{b}.  

In order to quantify the conceptual approach described in the previous paragraph we proceed by estimating the point intensity at each location. Denote the estimated point intensity at location $k$ by $\gamma_k$ and define a weight function 
\begin{equation}
	w_k = \left(\frac{1}{\gamma_k}\right)^p, 
\end{equation}
where $p$ is a scale parameter connected to the strength of dependence.
Let $\mathbf{W}=diag(\mathbf{w}_1, \dots, \mathbf{w}_n)$, where $\mathbf{w}_i$ is a row vector whose length is equal to the length of $\mathbf{b}^{(i)}$ and whose components are all equal to $w_i$. The covariance estimator is adjusted by defining the loss function in \eqref{kriging: cov est} as 
\begin{equation}
	l_{n}(C)= (\mathbf{b} - \mathbf{C})^T\mathbf{W}(\mathbf{b} - \mathbf{C}). \label{eq:diag weighted loss function} 
\end{equation}
Note that for independent data, the value $p = 0$ will give equal weights and result in an identity matrix, i.e.,$\mathbf{W} = \mathbf{I}$. In Section \ref{sub:simulation covariance} we conduct a simulation study aimed at identifying optimal choice of $p$ under various spatial dependence scenarios. 

% To produce smooth estimates of the trajectories $X_i(t)$, project the observations $Y_i(t_j), j = 1, \dots, m$ onto the finite dimensional functional basis $\{\hat{\psi}_k(t), k = 1, \dots, q\}$, where $q$ is chosen such that at least $90\%$ of the variation is accounted for. The fitted trajectories admit the following representation
% \begin{equation}
% 	\widehat{X_i(t)} = \sum_{k=1}^q\alpha_{k,i} \hat{\psi}_k(t) = \boldsymbol{\alpha_i'}\boldsymbol{\psi}.
% 	\label{phen:coef}
% \end{equation}
% In this representation the randomness associated with each random trajectory $X(t)$ is captured in the coefficient vector $\boldsymbol{\alpha}$. In Section \ref{sub:land_cover_classification} the coefficients are treated as features in a classification model for land cover type. 

% section cokriging_functional_principal_compents_scores (end)
% section functional_kriging_methods (end)


% (fold)
\subsubsection{Functional Principal Component Estimation} % (fold)
\label{sub:FPC estimation}

% subsection subsection_name (end)
One of the practical benefits of the covariance estimator in \eqref{kriging: cov est} is that closed form expressions for the principal component functions can be computed. The methodology we describe here was developed in Chapter \ref{ch:covariance estimation}.

Functional principal components are related to the well-known Karhunen-Loeve representation theorem. For a square-integrable stochastic process $X(t)$ defined on a closed interval $[a,b]$, with continuous covariance $C(t',t)$, there is a corresponding linear operator $[T_Cf](t') = \int_a^bC(t',t)f(t)dt$. Since $C(t',t)$ is symmetric and non-negative definite, it has the following representation 

%(see Mercer's theorem)
\begin{equation*}
	C(t',t) = \sum_{i=1}^{\infty}\lambda_i\psi_i(t')\psi_i(t), 
\end{equation*}
where $\{\psi_m(t)\}_{m=1,2,\ldots}$ are a sequence of orthonormal eigenfunctions which form a complete basis in $L^2[a,b]$, and $\{\lambda_m \}_{m=1,2,\ldots}$ are nonnegative and nondecreasing eigenvalues. The eigenfunction-eigenvalue pair $\{\lambda_j, \psi_j(t)\}$ satisfy $\int_a^bC(t',t)\psi_j(t)dt = \lambda_j\psi_j(t)$, in other words the FPCs $\{\psi_j(t)\}$ are eigenfunctions of the covariance operator. The Karhunen-Loeve theorem states that the process $X(t)$ admits the representation 
\begin{equation*}
	X(t) = \sum_{m=1}^{\infty}\alpha_m \psi_m(t), \mbox{ where } \alpha_m = \int_a^b X(t) \psi_m(t)dt, 
\end{equation*}
and the random variables $\{\alpha_m \}_{m=1,2,\ldots}$ are uncorrelated and satisfy $E(\alpha_m)=0$ and Var($\alpha_m$) = $\lambda_m$, $\sum_m \lambda_m < \infty$. The eigenfunctions $\{\psi_m(t)\}_{m=1,2,\ldots}$ corresponding to $C(t',t)$ are the principal component functions and the coefficients $\{\alpha_m \}$ are the functional principal component scores of $X(t)$.

We seek functions $\hat{\psi}(s)$ that satisfy satisfy 
\begin{equation}
	\label{eq:eigenfuns} \int \hat{C}(t',t)\hat{\psi}(t)dt=\theta\hat{\psi}(t'). \nonumber
\end{equation}
The following Lemma which is Lemma \ref{thm:eigenfunctions} in Chapter \ref{ch:covariance estimation} states that the eigenfunction are linear combinations of function derived from the reproducing kernel on $\H$. 
\begin{lemma}
	\label{thm:eigenfunctions2} The eigenfunctions of $\hat{C}(t',t)$ can be expressed as 
	\begin{equation*}
		\hat{\psi}_k(\cdot) = \mathbf{b}'_k\mathbf{g}(\cdot), 
	\end{equation*}
	where $b_k$ is the $k$-th column of $B=Q^{-1/2}U$ and $U$ is the eigenvectors of $Q^{1/2}AQ^{1/2}$, and
	\[ \mathbf{g(\cdot)}=(1, k_1(\cdot),R_{1}(\cdot, t_1),R_{1}(\cdot, t_2),\dots, R_{1}(\cdot, t_K))'. \]
\end{lemma}
The exact form of the reproducing kernel on $\H$ is described in \ref{ch:covariance estimation}. In the Lemma \ref{thm:eigenfunctions2} the value $K$ is connected to the number of knot locations used for covariance estimation. See Section \ref{sub:practical_considerations_for_knot_selection} for recommendations on knot selection.

% section eigenfunction_estimation (end)
\subsection{Prediction} 

% (fold)
\label{sec:functional_kriging_predictor}

Using the functional principal components representation of $X_s(t)$ from \eqref{kriging: fpc expansion},
\begin{equation}
	X_{s}(t) = \sum_{k=1}^{q} \alpha_k(s)\psi_k(t) = \boldsymbol{\alpha}(s)\boldsymbol{\psi}, 
\end{equation}
the objective of constructing the best linear unbiased predictor of $X_{s_0}(t)$ and unobserved location $s_0$ is reframed as constructing the best linear unbiased predictor of the vector $\balpha_{s_0}$ given $\balpha(s_1), \dots, \balpha(s_n)$. We model $\balpha(s)$ as a stationary random field. A linaer predictor of the multivariate data $\balpha(s_1), \dots, \balpha_{s_1}$ has the form
\begin{equation}
	\hat{\balpha}(s_0) = \sum_{i=1}^n \balpha(s_i)\Gamma_i \label{kriging:predictor}
\end{equation}
where $\Gamma_i$ is a $q \times q$ matrix with $ij^{th}$ element equal to $\lambda_{ij}$. 
\begin{equation}\label{kriging:predictor 2}
		\hat{\balpha}(s_0) = \sum_{i=1}^n [\alpha_1(s_i), \dots, \alpha_q(s_i)] 
		\left[ 
			\begin{array}{ccc}
				\lambda^i_{11} & \dots & \lambda^i_{1q}\\
				\vdots & \ddots & \vdots \\
				\lambda^i_{q1} & \dots & \lambda^i_{qq}
			\end{array}
		\right]
\end{equation}
From \eqref{kriging:predictor 2} it is straight forward to derive the form of the components of $\hat{\balpha}(s_0)=[\hat{\alpha}_1(s_0), \dots, \hat{\alpha}_q(s_0)]$
\begin{equation}
	\hat{\alpha}_k(s_0) = \sum_{i=1}^n\sum_{j=1}^q\alpha_j(s_i)\lambda^i_{jk}, \mbox{ } k = 1, \dots, q.
\end{equation}
The off-diagonal values, $\lambda^i_{jk}$ $j \neq k$, are connected to the cross-covariance of the scalar random fields $\alpha(s)$ and are all equal to zero for a process with no cross-covariance; that is, under the assumption that $Cov(\alpha_j(s), \alpha_k(s)) = 0$ for $j \neq k$
\begin{equation}
	\lambda^i_{jk} = \begin{cases}
														0 & \text{ if } j \neq k\\
														\sum_{jj}^i = 1 & \text{for each $j$}\\
 										\end{cases}
\end{equation}

This shows that for an isotopic process with no cross-correlation, the kriging predictor is equivalent to kriging the components (\cite{wackernagel2003multivariate}, Ch. 25). Thus the problem of kriging a function is equival to univariate kriging of scalar coefficients. 

% Steps for prediction at unobserved location $s_0$:
% \begin{enumerate}
% 	\item Estimate the principal component functions, $\hat{\psi}^{(1)}, \hat{\psi}^{(2)}, \dots, \hat{\psi}^{(q)}$, choosing $q$ to account for at least 90\% of the total variation.
% 	\item Compute the projection of sample curves onto principal component functions
% 	\item Perform standard scalar-valued geostatistical analysis by fitting variograms to each scalar coefficient field.
% 	\item Compute kriged estimates of the coefficients: $\hat{\alpha}_{s_0}^{(1)}, \hat{\alpha}_{s_0}^{(2)}, \cdots, \hat{\alpha}_{s_0}^{(q)}$ .
% 	\item The functional cokriging predictor of the curve at location $s_0$ is given by
% 	\begin{equation}
% 		\widehat{X}_{s_0}(t)_{ok} = \sum_{k=1}^{q} \hat{\alpha}_{s_0}^{(k)}\hat{\psi}^{(k)}(t).
% 	\end{equation}
	
	%\todo{describe variance of the estimator}
%\end{enumerate}


% section functional_kriging_predictor (end)

\section{Simulation Studies} 

% (fold)
\label{sec:numerical_experiments}

In this section we conduct investigations using simulated data. We generate functional data as
\begin{equation}\label{eq:sim process2} 
	X_s(t) = \sum^{3}_{k=1}\zeta_k Z_k(s) \cos(k\pi t) \hspace{0.5cm} t \in [0,1], 
\end{equation}
where \(\zeta_k=(-1)^{k+1}k^{-2}\). The random variables $Z_k(s)$ are sampled from a Gaussian random field with $E[Z_k(s)]=0$ and 
\begin{equation}\label{eq:exp cov}
	Cov(Z_k(s_i), Z_j(s_l)) = \begin{cases} 
																e^{-\norm{s_i - s_l}/r} &\mbox{ if } k = j\\
																0 & \mbox{ if } j \neq k.
															\end{cases}
\end{equation} 
The range parameter, $r$, in the exponential covariance in equation \eqref{eq:exp cov} controls the strength of spatial dependence. 

\subsection{Simulation study of the spatially weighted covariance function estimator} % (fold)
\label{sub:simulation covariance}

In this simulation study we investigate the effect of adjusting the covariance function estimator for spatial dependence by simulating curves from the three different sets of locations shown in Figure~\ref{fig:grid3}. Different degrees of spatial dependence are controlled by the range parameter $r$ in \eqref{eq:exp cov} of the Gaussian random fields, $Z_i$, in \eqref{eq:sim process2}. Figure~\ref{fig:exp_corr_funs} shows the exponential correlation functions that correspond to the range values: $r = 0.0, 0.1, 0.2, 0.3, 0.4$, ranging from independence (r = 0) to strong spatial dependence (r = 0.4).

In order to gain insight into the performance of the covariance estimator in \eqref{eq:diag weighted loss function} simulate functional data sets using the observation model 
\begin{equation}
	Y^{(k)}_{ij}(t_{ij}) = X_{ij}(t_{ij}) + \epsilon_{ij}, \mbox{ } i = 1, \dots, 68;\; j = 1, \dots, 20;\; k = 1, \dots, 100,
\end{equation}
where $\epsilon_{ij} \sim N(0, 0.01^2)$ and the superscript $k$ indexes simulated data sets. The number of observations per curve and observation error variance are fixed. 

For each simulated data set, $Y^{(k)}$, the integrated squared difference between the estimated and true covariance function is calculated. We define the quantity $L$ to be the average integrated squared difference
\begin{equation}
	L = \frac{1}{100}\sum_{k=1}^{100}\int_{[0,1]^2} [\hat{C}^{(k)}(t_1, t_2) - C(t_1,t_2)]^2dt_1dt_2.
\end{equation}
The results of the simulations are shown in Figure~\ref{fig:MSE_trends}, where value of $L$ for each simulation scenario is plotted with lines connecting points with the same strength of spatial dependence. The spatial weights make more of an improvement on estimation for curves with stronger spatial correlation, though the optimal choice of the weight parameter appears to be near $1/2$ for all scenarios. We also note that very little estimation performance is sacrificed by using the spatial weights for independent curves for $p \leq 1/2$.

% plot of locations used to simulate curves
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\textwidth]{Images-ordinary-kriging/Plots/grids.pdf} 
	\end{center}
	\isucaption{Locations of curves used for the simulation.} \label{fig:grid3} 
\end{figure}

% plot of exponential covariance functions
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{Images-ordinary-kriging/Plots/exp_corr_funs.pdf} 
	\end{center}
	\isucaption{Exponential covariance functions used in the simulation.} \label{fig:exp_corr_funs} 
\end{figure}

% plots of MSE vs spatial weight values for different values of spatial dependence
\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\textwidth]{Images-ordinary-kriging/Plots/MSE_trends_all.pdf} 
	\end{center}
	\isucaption{The x-axis shows the value of the scale parameter, $p$, in the weight function. Large values of $p$ correspond to smaller weights for curves in high point intensity areas. The y-axis shows the average integrated square error for the covariance estimator. The error bars show +/- two standard errors.} \label{fig:MSE_trends} 
\end{figure}

% sec:investigating_the_effects_of_spatial_dependence (end)

\subsection{Comparing prediction performance} % (fold)
\label{sub:comparing_prediction_performance}
This simulation study is designed to compare the prediction performance among methods for functional kriging. Figure \ref{fig:pred locations} shows a spatial grid of 68 locations from which spatially correlated data were simulated using an exponential covariance function with range $r = 0.2$ (see Figure \ref{fig:exp_corr_funs}). Prediction of functions at 12 unobserved locations was carried out for each of the following methods.
\begin{itemize}
	%\item IND: This method uses the overall mean as the prediction for each location.
	\item CFPC: Cokriging functional principal components. This is the (un-weighted) estimator developed in this paper.
	\item CFPCw: This is the method developed in this paper, but using the weighted covariance estimator with $p=0.5$.
	\item OKFD: Ordinary kriging of functional data method described in Section \ref{sec:ordinary_kriging_of_function_valued_data}.
\end{itemize}

Observed data consisted of $m = 10, 20$ observations from each curve with noise ($\sigma_0=0.1, 0.3$). The observed time points are the same for each curve for comparison purposes. This is due to the fact that the \texttt{geofd} R package (version 0.4.6) used for the OKFD method does not support different time points across curves. The OKFD methodology does allow for different time points across curves, so it is only the implementation that causes this restriction. It is unlikely that there will be future versions of the \texttt{geofd} package which support varying time points since the \texttt{geofd} package is no longer supported on CRAN and does not appear to be under active development. The implementation of CFPC method does allow for time points vary across curves, but we use fixed time points in the simulations in order to directly compare the performance of the different methods.                                              

%Figure~\ref{fig:curve kriging predictions} an example of predictions for one simulated data set. The numbers on the plots correspond to the numbered locations on the simulation grid (Figure \ref{fig:pred locations}). 
Boxplots summarizing 1000 simulated data sets for each scenario are shown in Figure~\ref{fig:boxplots pred mse}, where the measured variable is the average prediction mean squared error for the 12 prediction locations. The boxplots in Figure~\ref{fig:boxplots pred mse} show that the proposed method performs as good or better than the OKFD in term of prediction mean squared error. It also shows a clear difference between the prediction performance between weighted and un-weighted covariance estimator. None of the methods are uniformly better, but the values in Table \ref{tab:kriging_pred} and Table \ref{tab:kriging_pred_2} show that in three out of four of the high noise ($\sigma_0 = 0.3$) cases the weighted covariance estimator performs best.  
 
\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{Images-ordinary-kriging/Plots/pred_locations.pdf} 
	\end{center}
	\isucaption{Locations of curves generated for the simulation study. Black points show locations used for fitting each model. The 12 red numbers labeled on the plot show prediction locations.} \label{fig:pred locations} 
\end{figure}
%\begin{figure}
%	\begin{center}
%		\includegraphics[width=0.8\textwidth]{Images-ordinary-kriging/Plots/pred_curves_exp.pdf} 
%	\end{center}
%	\isucaption{Predicted curves at the 12 unobserved locations for a single simulated data set using the process \ref{eq:sim process2} with exponential covariance \eqref{eq:exp cov} $(r = 0.2; \sigma = 1 )$, 20 observations per curve, and observation error standard deviation $\sigma_0 = 0.3$. The true curves at prediction locations are shown as black solid lines. } \label{fig:curve kriging predictions} 
%\end{figure}
\begin{figure}
	\begin{center}
		\includegraphics[width=\textwidth]{Images-ordinary-kriging/Plots/kriging_boxplots_all.pdf} 
	\end{center}
	\isucaption{Boxplots showing distribution of the average prediction error, $\norm{\hat{X}(t) - X(t)}^2_{L_2}$, across the 12 unobserved locations shown in \ref{fig:pred locations} for 1000 simulated data sets. The curves were simulated using the functional process in \eqref{eq:sim process2} with the covariance function in \eqref{eq:exp cov}. The plot label `dependence' refers to the value of the range parameter, $r$, in the covariance function. The plot label `sigma' refers to the observation noise standard deviation $\sigma_0$ in \eqref{kriging:observation model}. } 
	\label{fig:boxplots pred mse} 
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% tables prediction MSE for various functional kriging methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
	\begin{center}
	\isucaption{Prediction error for the kriging predictors using exponential covariance function with $r = 0.2$. The number in parentheses is the standard error. }
		\begin{tabular}
			{|c|c|l|c|} \hline $\sigma_0$ & m & method & MSE \\
			\hline \multirow{6}{*}{0.01}& \multirow{3}{*}{10}
			  &CFPC&  0.230  (0.011)  \\
			& &CFPCw& 0.218  (0.010)  \\
			& &OKFD& 0.198  (0.009) \\
			\cline{2-4} & \multirow{3}{*}{20}
			  &CFPC& 0.206  (0.009) \\
			& &CFPCw&  0.188  (0.008)  \\
			& &OKFD& 0.182  (0.008)  \\
			\hline \multirow{6}{*}{0.3}& \multirow{3}{*}{10}
			  &CFPC& 0.233  (0.011)   \\
			& &CFPCw& 0.210  (0.008) \\
			& &OKFD& 0.214  (0.009)  \\
			\cline{2-4} & \multirow{3}{*}{20}
			  &CFPC& 0.227  (0.011)  \\
			& &CFPCw& 0.222  (0.009)  \\
			& &OKFD& 0.225  (0.010)  \\
			\hline
		\end{tabular}
	\label{tab:kriging_pred}
	\end{center}
\end{table}

\begin{table}
	\begin{center}
	\isucaption{Prediction error for the kriging predictors using exponential covariance function with r = 0.3. The number in parentheses is the standard error.}
		\begin{tabular}{|c|c|l|c|} \hline
			$\sigma_0$ & m & method & MSE \\
			\hline
			\multirow{6}{*}{0.01}& \multirow{3}{*}{10}
			  &CFPC&  0.153  (0.007)  \\
			& &CFPCw& 0.153  (0.007)  \\
			& &OKFD& 0.138  (0.006) \\
			\cline{2-4} & \multirow{3}{*}{20}
			  &CFPC& 0.138  (0.007) \\
			& &CFPCw&  0.144  (0.006)  \\
			& &OKFD& 0.137  (0.006) \\
			\hline \multirow{6}{*}{0.3}& \multirow{3}{*}{10}
			  &CFPC& 0.164  (0.006)  \\
			& &CFPCw& 0.149  (0.006) \\
			& &OKFD& 0.155  (0.006) \\
			\cline{2-4} & \multirow{3}{*}{20}
			  &CFPC& 0.151  (0.006)  \\
			& &CFPCw& 0.141  (0.005) \\
			& &OKFD& 0.135  (0.005)  \\
			\hline
		\end{tabular}
	\label{tab:kriging_pred_2}
	\end{center}
\end{table}

% subsection comparing_prediction_performance (end)

\section{Discussion} % (fold)
\label{sec:discussion}
 We have developed a parsimonious approach to functional kriging by exploiting a low dimensional representation of curves through a functional principal component basis. By assuming no cross-correlation among vectors of coefficients, the practitioner is only required to model a small number of scalar random fields. This is a computationally attractive property, but relies heavily on efficient estimation of functional principal components from spatially dependent curves.  The proposed method achieves efficient estimation of functional principal components by utilizing optimal covariance function estimation properties inherent to RKHS function spaces, and by introduced a spatial re-weighted estimator that tempers the effect of spatially dependent observations. 

The re-weighted covariance function estimator depends on a weight parameter, $p$, but the simulations conducted herein are encouraging for two reasons: (i) they confirm that the spatially re-weighted estimator can produce meaningful reductions in mean squared error for covariance function estimation, and (ii) that optimal selection of the scale parameter $p$ is not critical because the estimator achieves near-optimal reduction in mean squared error for values of $p$ between 1/3 and 1/2 for varying strengths of spatial dependence. Moreover, for the cases with the most highly correlated data ($r = 0.4$), the optimal value of $p$ is consistently near 1/2. Practically speaking, this means the value $p = 1/2$ can be used as an approximate minimax estimate. 

In terms of spatial prediction, the proposed functional kriging method performs similarly to standard methods for functional kriging, and performs slightly better when the functional process is observed with noise. The weighted version of the covariance estimator not only improves covariance estimation in the presence of spatial dependence, but improves spatial prediction compared to the unweighted version by achieving lower prediction error in all but one of the simulation cases used. 

Finally, recall that in order to make a direct comparison among the functional kriging methods it was necessary to use the same observed time points for each curve due to the fact the the OKFD implementation in the \texttt{geofd} R package only supports the fixed time point case. The theoretical development of the CFPC method assumes time points for each curve follow a probability distribution, and hence the optimal theoretical properties need not hold in the case of fixed time points. When the same time points are used for each curve, the pooled data used in the covariance estimator in equation~\eqref{kriging: cov est} contain far fewer unique points in the product space. This means that in the more general case of random time points, the prediction performance for the CFPC and CFPCw methods will likely improve more than the OKFD method. In future work, we plan on modifying the OKFD implementation to allow for the more general case of random time points in order to gain this additional insight. 

% It is interesting to note that \cite{Nerini:2010ba} also used a RKHS framework for functional kriging, but for a different reason. They investigate a predictor of a functional observation at location $s_0$ defined to be a sum of the form
% \begin{equation}
% 	\widehat{X_{s_0}} = \sum_i^n B(X_i) \nonumber
% \end{equation}
% where $B_i: \H \rightarrow \H$ are linear operators. In order for this estimator to be an unbiased estimator the linear operators $B_i$ must satisfy
% \begin{equation}
% 	\left[\sum_i^n B_i\right](\mu) = \mu, \nonumber
% \end{equation}
% which in the ordinary kriging setting, where $\mu$ is unknown, further requires the following
% \begin{equation}
% 	\left[\sum_i^n B_i\right](f) = f, \forall f \in \H. \nonumber
% \end{equation}
% The previous condition is satisfied for function spaces to which an operator $K$ exists that satisfies $K(f) = f, \forall f\in \H$, which is connected to properties of RKHS. In this case the assumption of an RKHS is a regularity condition guaranteeing the existence of an unbiased estimator, whereas our motivation for the use of RKHS is efficient estimation of functional principal components. 
% section discussion (end)

%\todo[inline]{discuss how possible measures of interest (e.g. min, max, average) are functionals of the curve predictor. Should this go in the introduction? }
